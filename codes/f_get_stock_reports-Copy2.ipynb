{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------\n",
      "2016-08-29 23:51:36.073841\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "start_time = datetime.now()\n",
    "\n",
    "print('-' * len(str(start_time)))\n",
    "print(str(datetime.now()) + '\\n')\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import urllib\n",
    "import pandas as pd\n",
    "import os #文件系统\n",
    "import shutil #文件复制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sw_industry_list(industry_code): # 不用 list ，用 text 输入\n",
    "    \n",
    "    # 引入所需包\n",
    "    from bs4 import BeautifulSoup\n",
    "    import requests\n",
    "    import urllib\n",
    "    import pandas as pd\n",
    "    \n",
    "    # 读取数据文件\n",
    "    html_doc = urllib.request.urlopen('http://www.swsindex.com/downfile.aspx?code=' + industry_code).read()\n",
    "    \n",
    "    # 用 lmxl 解析数据文件\n",
    "    soup0 = BeautifulSoup(html_doc, \"lxml\")\n",
    "    \n",
    "    # 分离出 table\n",
    "    table1 = soup0.find_all('table')[0]\n",
    "    \n",
    "    # 准备 list1、list2 两个空 list 备用\n",
    "    list1 = []\n",
    "    list2 = []\n",
    "    \n",
    "    # 处理 th 即标题\n",
    "    \n",
    "    for i in range(0, len(table1.find_all('th'))):\n",
    "        list2.append(table1.find_all('th')[i].text)\n",
    "        \n",
    "    # 将标题存入 list1\n",
    "    list1.append(list2)\n",
    "    \n",
    "    # 再次将 list2 清空\n",
    "    list2 = []\n",
    "    \n",
    "    # 第一层循环，依次读取每一行 tr\n",
    "    for i in range(1, len(table1.find_all('tr'))):\n",
    "        \n",
    "        # 每次将 list2 清空备用\n",
    "        list2 = []\n",
    "        \n",
    "        # 第二层循环，读取每个 td 元素，其 text 依次存入 list2\n",
    "        for j in table1.find_all('tr')[i].find_all('td'):\n",
    "            list2.append(j.text)\n",
    "            \n",
    "        # 将写入的 list2 文件附加到 list1,循环完成即生成包函完整数据的 list1\n",
    "        list1.append(list2)\n",
    "        \n",
    "    # 整理生成的 DataFrame\n",
    "    datatemp = pd.DataFrame(list1[1:], columns = list1[0])\n",
    "    datatemp.index = datatemp['证券代码']\n",
    "    datatemp.index.name = 'code'\n",
    "    datatemp['文件夹'] = '/home/wangshi/reports/stock_reports/' + industry_code + '/' + datatemp['证券代码'] + '_' + datatemp['证券名称'] + '/'\n",
    "    \n",
    "    for i in list(datatemp['文件夹']):\n",
    "        if os.path.exists(i):  \n",
    "            pass  \n",
    "        else:  \n",
    "            os.mkdir(i)  \n",
    "    \n",
    "    # 返回股票清单 DataFrame, 后用 list 函数返回\n",
    "    return datatemp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DataFrame 的差\n",
    "def df_diff(df_a, df_b):\n",
    "    df_a_b = df_a.ix[df_a.index.difference(df_b.index)]\n",
    "    return df_a_b\n",
    "\n",
    "# DataFrame 的并\n",
    "def df_all(df_a, df_b):\n",
    "    df_a_b = df_a.ix[df_a.index.difference(df_b.index)]\n",
    "    df_all = df_a_b.append(df_b)\n",
    "    return df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 此处用 list 输入 codes，可试用 list + DataFrame 输入\n",
    "def get_eastmoney_stock_report(urls, paths, code, name):\n",
    "    \n",
    "    # 下载个股\n",
    "    from dateutil.parser import parse\n",
    "    from bs4 import BeautifulSoup\n",
    "    import requests, urllib, os, shutil, json\n",
    "    import pandas as pd\n",
    "    \n",
    "    # 此处待添加验证 urls 是否为 DataFrame\n",
    "    count_download = 0\n",
    "    count_pass = 0\n",
    "    count_fail = 0\n",
    "    for i in range(0, len(urls)):\n",
    "        temp_url_3 = 'http://data.eastmoney.com/report/' + parse(urls.loc[urls.index[i]]['datetime']).strftime('%Y'+'%m'+'%d') + '/' + urls.loc[urls.index[i]]['infoCode'] + '.html'\n",
    "        # print(temp_url_3)\n",
    "        html_doc_3 = urllib.request.urlopen(temp_url_3).read()\n",
    "        soup_3 = BeautifulSoup(html_doc_3, \"lxml\")\n",
    "        try:\n",
    "            file_url_3 = soup_3.find_all(text = '查看PDF原文')[0].parent.get('href')\n",
    "            temp_name_3 = paths + parse(urls.loc[urls.index[i]]['datetime']).strftime('%g'+'%m'+'%d') + '_' + code + '_' + name + '_' + urls.loc[urls.index[i]]['insName'] + '_' + urls.loc[urls.index[i]]['title'] + '.pdf'\n",
    "        \n",
    "            if os.path.isfile(temp_name_3) == True:\n",
    "                # print('File already exist! PASS!')\n",
    "                count_pass += 1\n",
    "                pass\n",
    "            else:\n",
    "                urllib.request.urlretrieve(file_url_3, temp_name_3)\n",
    "                count_download += 1\n",
    "                # print('Download ' + str(count_download) + '/' + str(len(urls)) + ' new files '+ 'Successfully !')\n",
    "        except:\n",
    "            print('Fail to get the file. ' + 'Please download the file manually !' + '\\n'  + temp_url_3)\n",
    "            count_fail += 1\n",
    "            pass\n",
    "        \n",
    "    print(code + '_' + name + '：' + 'DOWN:' + str(count_download) + '_' + 'PASS:' + str(count_pass) + '_' + 'FAIL:' + str(count_fail))\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 通过输入股票代码列表下载股票报告清单到硬盘\n",
    "def get_stock_report_list(DataFrame, path = '/home/wangshi/script/stocks_reports_list/'):\n",
    "    # /home/wangshi/script/stocks_reports_list/\n",
    "    \n",
    "    from bs4 import BeautifulSoup\n",
    "    from math import ceil\n",
    "    import requests, urllib, os, shutil, json\n",
    "    import pandas as pd\n",
    "    \n",
    "    \n",
    "    # 第一层循环，遍历列表中每个股票代码，取得其报告页数\n",
    "    for stock_code in list(DataFrame['证券代码']):\n",
    "        \n",
    "        try:\n",
    "            url_data_0 = 'http://datainterface.eastmoney.com//EM_DataCenter/js.aspx?type=SR&sty=GGSR&js=var%20PrnJnSby={%22data%22:[(x)],%22pages%22:%22(pc)%22,%22update%22:%22(ud)%22,%22count%22:%22(count)%22}&ps=25&p=1&code=' + stock_code\n",
    "            html_doc_0 = urllib.request.urlopen(url_data_0).read()\n",
    "            soup_0 = BeautifulSoup(html_doc_0, \"lxml\")\n",
    "        \n",
    "            # 用 json 处理得到的 json 文本\n",
    "            jsontext_0 = json.loads(soup_0.text.split('=')[1])\n",
    "        \n",
    "            # 向上取整取得报告页数\n",
    "            page_numbers = ceil(int(jsontext_0['count'])/25)\n",
    "        \n",
    "            data_0 = []\n",
    "        \n",
    "            # 第二层循环，遍历股票报告页面，取得其报告编码\n",
    "            for page_number in range(1, page_numbers + 1):\n",
    "            \n",
    "                url_data_1 = 'http://datainterface.eastmoney.com//EM_DataCenter/js.aspx?type=SR&sty=GGSR&js=var%20PrnJnSby={%22data%22:[(x)],%22pages%22:%22(pc)%22,%22update%22:%22(ud)%22,%22count%22:%22(count)%22}&ps=25&p=' + str(page_number) + '&code=' + stock_code\n",
    "                html_doc_1 = urllib.request.urlopen(url_data_1).read()\n",
    "                soup_1 = BeautifulSoup(html_doc_1, \"lxml\")\n",
    "            \n",
    "                # 用 json 处理得到的 json 文本\n",
    "                jsontext_1 = json.loads(soup_1.text.split('=')[1])\n",
    "            \n",
    "                #合并新的列表\n",
    "                data_0 += jsontext_1['data']\n",
    "        \n",
    "            #处理取得的编码为DataFrame，并将结果存盘\n",
    "            data_1 = pd.DataFrame(data_0)\n",
    "            data_1.index = data_1['infoCode']\n",
    "            data_1.index.name = 'infoCodes' # 为避免名称重复这里设置为 infoCodes\n",
    "            \n",
    "            # 识别旧数据是否存在\n",
    "            if os.path.isfile(path + stock_code) == True:\n",
    "                data_old = pd.read_csv(path + stock_code, index_col = 'infoCodes')\n",
    "                data_delta = df_diff(data_1, data_old)\n",
    "\n",
    "            else:\n",
    "                data_delta = data_1\n",
    "               \n",
    "            \n",
    "            get_eastmoney_stock_report(data_delta, DataFrame['文件夹'].loc[stock_code], DataFrame['证券代码'].loc[stock_code], DataFrame['证券名称'].loc[stock_code])\n",
    "            \n",
    "            data_1.to_csv(path + stock_code)\n",
    "        \n",
    "        except:\n",
    "            print('Fail to get reports of ' + DataFrame['证券代码'].loc[stock_code] + '_' + DataFrame['证券名称'].loc[stock_code])\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "td = get_sw_industry_list('801120')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to get reports of 000019\n",
      "Fail to get reports of 000557\n",
      "000568_泸州老窖：DOWN:0_PASS:107_FAIL:0\n",
      "000596_古井贡酒：DOWN:0_PASS:58_FAIL:0\n",
      "000716_黑芝麻：DOWN:0_PASS:17_FAIL:0\n",
      "000729_燕京啤酒：DOWN:0_PASS:27_FAIL:0\n",
      "Fail to get reports of 000752\n",
      "000799_酒鬼酒：DOWN:0_PASS:14_FAIL:0\n",
      "000848_承德露露：DOWN:0_PASS:58_FAIL:0\n",
      "000858_五粮液：DOWN:0_PASS:140_FAIL:0\n",
      "000860_顺鑫农业：DOWN:0_PASS:81_FAIL:0\n",
      "000869_张裕A：DOWN:0_PASS:40_FAIL:0\n",
      "Fail to get the file. Please download the file manually !\n",
      "http://data.eastmoney.com/report/20160823/APPH531J7ry6ASearchReport.html\n",
      "000895_双汇发展：DOWN:114_PASS:1_FAIL:1\n",
      "Fail to get reports of 000929\n",
      "Fail to get reports of 000995\n",
      "002216_三全食品：DOWN:0_PASS:0_FAIL:0\n",
      "Fail to get reports of 002304\n",
      "002329_皇氏集团：DOWN:37_PASS:9_FAIL:0\n",
      "002330_得利斯：DOWN:5_PASS:0_FAIL:0\n",
      "002387_黑牛食品：DOWN:20_PASS:0_FAIL:0\n",
      "002461_珠江啤酒：DOWN:1_PASS:0_FAIL:0\n",
      "002481_双塔食品：DOWN:26_PASS:0_FAIL:0\n",
      "002495_佳隆股份：DOWN:1_PASS:0_FAIL:0\n",
      "002507_涪陵榨菜：DOWN:22_PASS:0_FAIL:0\n",
      "002515_金字火腿：DOWN:2_PASS:0_FAIL:0\n",
      "Fail to get reports of 002557\n",
      "002568_百润股份：DOWN:58_PASS:0_FAIL:0\n",
      "002570_贝因美：DOWN:44_PASS:0_FAIL:0\n",
      "002582_好想你：DOWN:64_PASS:0_FAIL:0\n",
      "002646_青青稞酒：DOWN:26_PASS:0_FAIL:0\n",
      "002650_加加食品：DOWN:56_PASS:0_FAIL:0\n",
      "002661_克明面业：DOWN:73_PASS:0_FAIL:0\n",
      "002695_煌上煌：DOWN:22_PASS:0_FAIL:0\n",
      "Fail to get reports of 002702\n",
      "002719_麦趣尔：DOWN:11_PASS:0_FAIL:0\n",
      "002726_龙大肉食：DOWN:7_PASS:0_FAIL:0\n",
      "002732_燕塘乳业：DOWN:4_PASS:0_FAIL:0\n",
      "002770_科迪乳业：DOWN:5_PASS:0_FAIL:0\n",
      "Fail to get the file. Please download the file manually !\n",
      "http://data.eastmoney.com/report/20160803/APPH4xGYqiMxASearchReport.html\n",
      "Fail to get the file. Please download the file manually !\n",
      "http://data.eastmoney.com/report/20160619/APPH4DdrTLAzASearchReport.html\n",
      "Fail to get the file. Please download the file manually !\n",
      "http://data.eastmoney.com/report/20160317/APPH313dAqZ4ASearchReport.html\n",
      "Fail to get the file. Please download the file manually !\n",
      "http://data.eastmoney.com/report/20160303/APPH2winUWdhASearchReport.html\n",
      "Fail to get the file. Please download the file manually !\n",
      "http://data.eastmoney.com/report/20151008/APPGPb7374tbASearchReport.html\n",
      "Fail to get the file. Please download the file manually !\n",
      "http://data.eastmoney.com/report/20150901/APPH2FEzZ2tVASearchReport.html\n",
      "300146_汤臣倍健：DOWN:71_PASS:0_FAIL:6\n",
      "600059_古越龙山：DOWN:20_PASS:0_FAIL:0\n",
      "600073_上海梅林：DOWN:49_PASS:0_FAIL:0\n",
      "600084_中葡股份：DOWN:1_PASS:0_FAIL:0\n",
      "600090_同济堂：DOWN:1_PASS:0_FAIL:0\n",
      "Fail to get reports of 600132\n",
      "600186_莲花健康：DOWN:1_PASS:0_FAIL:0\n",
      "600197_伊力特：DOWN:7_PASS:0_FAIL:0\n",
      "600199_金种子酒：DOWN:7_PASS:0_FAIL:0\n",
      "600238_海南椰岛：DOWN:4_PASS:0_FAIL:0\n",
      "600300_维维股份：DOWN:6_PASS:0_FAIL:0\n",
      "600305_恒顺醋业：DOWN:76_PASS:0_FAIL:0\n",
      "Fail to get reports of 600365\n",
      "Fail to get reports of 600381\n",
      "600429_三元股份：DOWN:16_PASS:0_FAIL:0\n",
      "Fail to get reports of 600519\n",
      "600543_莫高股份：DOWN:1_PASS:0_FAIL:0\n",
      "600559_老白干酒：DOWN:41_PASS:0_FAIL:0\n",
      "Fail to get reports of 600573\n",
      "600597_光明乳业：DOWN:53_PASS:0_FAIL:0\n",
      "600600_青岛啤酒：DOWN:36_PASS:0_FAIL:0\n",
      "600616_金枫酒业：DOWN:7_PASS:0_FAIL:0\n",
      "Fail to get the file. Please download the file manually !\n",
      "http://data.eastmoney.com/report/20140819/APPFheXSCHrTASearchReport.html\n",
      "600702_沱牌舍得：DOWN:30_PASS:0_FAIL:1\n",
      "Fail to get reports of 600779\n",
      "600809_山西汾酒：DOWN:79_PASS:0_FAIL:0\n",
      "600866_*ST星湖：DOWN:1_PASS:0_FAIL:0\n",
      "600872_中炬高新：DOWN:104_PASS:0_FAIL:0\n",
      "600873_梅花生物：DOWN:44_PASS:0_FAIL:0\n",
      "600887_伊利股份：DOWN:143_PASS:0_FAIL:0\n",
      "601579_会稽山：DOWN:8_PASS:0_FAIL:0\n",
      "603020_爱普股份：DOWN:19_PASS:0_FAIL:0\n",
      "603027_千禾味业：DOWN:9_PASS:0_FAIL:0\n",
      "603031_安德利：DOWN:1_PASS:0_FAIL:0\n",
      "603198_迎驾贡酒：DOWN:9_PASS:0_FAIL:0\n",
      "603288_海天味业：DOWN:115_PASS:1_FAIL:0\n",
      "603369_今世缘：DOWN:15_PASS:0_FAIL:0\n",
      "603589_口子窖：DOWN:10_PASS:0_FAIL:0\n",
      "603696_安记食品：DOWN:8_PASS:0_FAIL:0\n",
      "603779_威龙股份：DOWN:3_PASS:0_FAIL:0\n",
      "603866_桃李面包：DOWN:21_PASS:0_FAIL:0\n",
      "603919_金徽酒：DOWN:7_PASS:0_FAIL:0\n",
      "\n",
      "USED 3134 seconds!\n",
      "\n",
      "2016-08-30 00:43:50.711698\n"
     ]
    }
   ],
   "source": [
    "get_stock_report_list(td)\n",
    "end_time = datetime.now()\n",
    "print('\\n' + 'USED ' + str((end_time - start_time).seconds) + ' seconds!')\n",
    "print('\\n' + str(datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# td = get_sw_industry_list('801120')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# path = '/home/wangshi/script/stocks_reports_list/'\n",
    "# data_0 = pd.read_csv(path + '000716', index_col = 'infoCodes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get_eastmoney_stock_report(data_0, '/home/wangshi/script/stocks_reports_list/industry_lists/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
