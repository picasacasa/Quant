{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------\n",
      "2016-09-02 00:55:07.389290\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "start_time = datetime.now()\n",
    "\n",
    "print('-' * len(str(start_time)))\n",
    "print(str(datetime.now()) + '\\n')\n",
    "\n",
    "from dateutil.parser import parse\n",
    "from bs4 import BeautifulSoup\n",
    "import requests, urllib, os, shutil, json\n",
    "import pandas as pd\n",
    "\n",
    "import traceback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sw_industry_list(industry_code): # 不用 list ，用 text 输入\n",
    "    \n",
    "    # 引入所需包\n",
    "    # from bs4 import BeautifulSoup\n",
    "    # import requests\n",
    "    # import urllib\n",
    "    # import pandas as pd\n",
    "    \n",
    "    # 读取数据文件\n",
    "    html_doc = urllib.request.urlopen('http://www.swsindex.com/downfile.aspx?code=' + industry_code).read()\n",
    "    \n",
    "    # 用 lmxl 解析数据文件\n",
    "    soup0 = BeautifulSoup(html_doc, \"lxml\")\n",
    "    \n",
    "    # 分离出 table\n",
    "    table1 = soup0.find_all('table')[0]\n",
    "    \n",
    "    # 准备 list1、list2 两个空 list 备用\n",
    "    list1 = []\n",
    "    list2 = []\n",
    "    \n",
    "    # 处理 th 即标题\n",
    "    \n",
    "    for i in range(0, len(table1.find_all('th'))):\n",
    "        list2.append(table1.find_all('th')[i].text)\n",
    "        \n",
    "    # 将标题存入 list1\n",
    "    list1.append(list2)\n",
    "    \n",
    "    # 再次将 list2 清空\n",
    "    list2 = []\n",
    "    \n",
    "    # 第一层循环，依次读取每一行 tr\n",
    "    for i in range(1, len(table1.find_all('tr'))):\n",
    "        \n",
    "        # 每次将 list2 清空备用\n",
    "        list2 = []\n",
    "        \n",
    "        # 第二层循环，读取每个 td 元素，其 text 依次存入 list2\n",
    "        for j in table1.find_all('tr')[i].find_all('td'):\n",
    "            list2.append(j.text)\n",
    "            \n",
    "        # 将写入的 list2 文件附加到 list1,循环完成即生成包函完整数据的 list1\n",
    "        list1.append(list2)\n",
    "        \n",
    "    # 整理生成的 DataFrame\n",
    "    datatemp = pd.DataFrame(list1[1:], columns = list1[0])\n",
    "    datatemp.index = datatemp['证券代码']\n",
    "    datatemp.index.name = 'code'\n",
    "    datatemp['文件夹'] = '/home/wangshi/reports/stock_reports/' + industry_code + '/' + datatemp['证券代码'] + '_' + datatemp['证券名称'] + '/'\n",
    "    \n",
    "    for i in list(datatemp['文件夹']):\n",
    "        if os.path.exists(i):  \n",
    "            pass  \n",
    "        else:  \n",
    "            os.mkdir(i)  \n",
    "    \n",
    "    # 返回股票清单 DataFrame, 后用 list 函数返回\n",
    "    return datatemp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DataFrame 的差\n",
    "def df_diff(df_a, df_b):\n",
    "    df_a_b = df_a.ix[df_a.index.difference(df_b.index)]\n",
    "    return df_a_b\n",
    "\n",
    "# DataFrame 的并\n",
    "def df_all(df_a, df_b):\n",
    "    df_a_b = df_a.ix[df_a.index.difference(df_b.index)]\n",
    "    df_all = df_a_b.append(df_b)\n",
    "    return df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 此处用 list 输入 codes，可试用 list + DataFrame 输入\n",
    "def get_eastmoney_stock_report(urls, paths, code, name):\n",
    "    \n",
    "    # 下载个股\n",
    "    # from dateutil.parser import parse\n",
    "    # from bs4 import BeautifulSoup\n",
    "    # import requests, urllib, os, shutil, json\n",
    "    # import pandas as pd\n",
    "    \n",
    "    # 此处待添加验证 urls 是否为 DataFrame\n",
    "    count_download = 0\n",
    "    count_pass = 0\n",
    "    count_fail = 0\n",
    "    for i in range(0, len(urls)):\n",
    "        try:\n",
    "            temp_url_3 = 'http://data.eastmoney.com/report/' + parse(urls.loc[urls.index[i]]['datetime'])[:10].strftime('%Y'+'%m'+'%d') + '/' + urls.loc[urls.index[i]]['infoCode'] + '.html'\n",
    "            \n",
    "            html_doc_3 = urllib.request.urlopen(temp_url_3).read()\n",
    "            soup_3 = BeautifulSoup(html_doc_3, \"lxml\")\n",
    "\n",
    "            file_url_3 = soup_3.find_all(text = '查看PDF原文')[0].parent.get('href')\n",
    "            temp_name_3 = paths + parse(urls.loc[urls.index[i]]['datetime']).strftime('%g'+'%m'+'%d') + '_' + code + '_' + name + '_' + urls.loc[urls.index[i]]['insName'] + '_' + urls.loc[urls.index[i]]['title'] + '.pdf'\n",
    "        \n",
    "            if os.path.isfile(temp_name_3) == True:\n",
    "                # print('File already exist! PASS!')\n",
    "                count_pass += 1\n",
    "                pass\n",
    "            else:\n",
    "                urllib.request.urlretrieve(file_url_3, temp_name_3)\n",
    "                count_download += 1\n",
    "                # print('Download ' + str(count_download) + '/' + str(len(urls)) + ' new files '+ 'Successfully !')\n",
    "        except:\n",
    "            print('Fail to get the file of ' + code + '_' + name + ',' + 'Please download the file manually !' + '\\n'  + temp_url_3)\n",
    "            count_fail += 1\n",
    "            pass\n",
    "        \n",
    "    if count_download == 0 and count_pass == 0 and count_fail == 0:\n",
    "        pass\n",
    "    \n",
    "    else:\n",
    "        print(code + '_' + name + '：' + 'DOWN:' + str(count_download) + '_' + 'PASS:' + str(count_pass) + '_' + 'FAIL:' + str(count_fail))\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 通过输入股票代码列表下载股票报告清单到硬盘\n",
    "def get_stock_report_list(DataFrame, path = '/home/wangshi/script/stocks_reports_list/'):\n",
    "    # /home/wangshi/script/stocks_reports_list/\n",
    "    \n",
    "    # from bs4 import BeautifulSoup\n",
    "    from math import ceil\n",
    "    # import requests, urllib, os, shutil, json\n",
    "    # import pandas as pd\n",
    "    \n",
    "    # 总计输出备用\n",
    "    reports_not_exist_count = 'Reports_Not_Exist:' + '\\n'\n",
    "    \n",
    "    # 第一层循环，遍历列表中每个股票代码，取得其报告页数\n",
    "    for stock_code in list(DataFrame['证券代码']):\n",
    "           \n",
    "        try:\n",
    "            url_data_0 = 'http://datainterface.eastmoney.com//EM_DataCenter/js.aspx?type=SR&sty=GGSR&js=var%20PrnJnSby={%22data%22:[(x)],%22pages%22:%22(pc)%22,%22update%22:%22(ud)%22,%22count%22:%22(count)%22}&ps=25&p=1&code=' + stock_code\n",
    "            html_doc_0 = urllib.request.urlopen(url_data_0).read()\n",
    "            soup_0 = BeautifulSoup(html_doc_0, \"lxml\")\n",
    "\n",
    "            # 用 json 处理得到的 json 文本\n",
    "            jsontext_0 = json.loads(soup_0.text.split('=', 1)[1].replace('[{stats:false}]','\"[{stats:false}]\"'))\n",
    "            \n",
    "            # 向上取整取得报告页数\n",
    "            if jsontext_0['data'] == '[{stats:false}]':\n",
    "                reports_not_exist_count += stock_code + '_' + DataFrame['证券名称'].loc[stock_code] + '\\n'\n",
    "                # print(stock_code + '_' + DataFrame['证券名称'].loc[stock_code] + ' Reports Not Exist')\n",
    "                pass\n",
    "                \n",
    "            else:\n",
    "                page_numbers = ceil(int(jsontext_0['count'])/25)\n",
    "        \n",
    "                data_0 = []\n",
    "        \n",
    "                # 第二层循环，遍历股票报告页面，取得其报告编码\n",
    "                for page_number in range(1, page_numbers + 1):\n",
    "            \n",
    "                    url_data_1 = 'http://datainterface.eastmoney.com//EM_DataCenter/js.aspx?type=SR&sty=GGSR&js=var%20PrnJnSby={%22data%22:[(x)],%22pages%22:%22(pc)%22,%22update%22:%22(ud)%22,%22count%22:%22(count)%22}&ps=25&p=' + str(page_number) + '&code=' + stock_code\n",
    "                    html_doc_1 = urllib.request.urlopen(url_data_1).read()\n",
    "                    soup_1 = BeautifulSoup(html_doc_1, \"lxml\")\n",
    "            \n",
    "                    # 用 json 处理得到的 json 文本\n",
    "                    jsontext_1 = json.loads(soup_1.text.split('=', 1)[1].replace('[{stats:false}]','\"[{stats:false}]\"'))\n",
    "            \n",
    "                    #合并新的列表\n",
    "                    data_0 += jsontext_1['data']\n",
    "        \n",
    "                #处理取得的编码为DataFrame，并将结果存盘\n",
    "                data_1 = pd.DataFrame(data_0)\n",
    "                data_1.index = data_1['infoCode']\n",
    "                data_1.index.name = 'infoCodes' # 为避免名称重复这里设置为 infoCodes\n",
    "            \n",
    "                # 识别旧数据是否存在\n",
    "                if os.path.isfile(path + stock_code) == True:\n",
    "                    data_old = pd.read_csv(path + stock_code, index_col = 'infoCodes')\n",
    "                    data_delta = df_diff(data_1, data_old)\n",
    "    \n",
    "                else:\n",
    "                    data_delta = data_1\n",
    "               \n",
    "            \n",
    "                    get_eastmoney_stock_report(data_delta, DataFrame['文件夹'].loc[stock_code], DataFrame['证券代码'].loc[stock_code], DataFrame['证券名称'].loc[stock_code])\n",
    "            \n",
    "                    data_1.to_csv(path + stock_code)\n",
    "        \n",
    "        except:\n",
    "            traceback.print_exc()\n",
    "            print('Fail to get reports of ' + DataFrame['证券代码'].loc[stock_code] + '_' + DataFrame['证券名称'].loc[stock_code])\n",
    "            \n",
    "    # 输出空值文件\n",
    "    print('\\n' +reports_not_exist_count + '\\n')\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "industry_codes = ['801120', '801150']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reports_Not_Exist:\n",
      "000019_深深宝A\n",
      "000557_西部创业\n",
      "000752_西藏发展\n",
      "000929_兰州黄河\n",
      "000995_*ST皇台\n",
      "600132_重庆啤酒\n",
      "600365_通葡股份\n",
      "600381_ST春天\n",
      "600573_惠泉啤酒\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-4-9480939d1263>\", line 16, in get_eastmoney_stock_report\n",
      "    temp_url_3 = 'http://data.eastmoney.com/report/' + parse(urls.loc[urls.index[i]]['datetime'])[:10].strftime('%Y'+'%m'+'%d') + '/' + urls.loc[urls.index[i]]['infoCode'] + '.html'\n",
      "  File \"/home/wangshi/anaconda3/lib/python3.5/site-packages/dateutil/parser.py\", line 1164, in parse\n",
      "    return DEFAULTPARSER.parse(timestr, **kwargs)\n",
      "  File \"/home/wangshi/anaconda3/lib/python3.5/site-packages/dateutil/parser.py\", line 552, in parse\n",
      "    res, skipped_tokens = self._parse(timestr, **kwargs)\n",
      "  File \"/home/wangshi/anaconda3/lib/python3.5/site-packages/dateutil/parser.py\", line 671, in _parse\n",
      "    l = _timelex.split(timestr)         # Splits the timestr into tokens\n",
      "  File \"/home/wangshi/anaconda3/lib/python3.5/site-packages/dateutil/parser.py\", line 188, in split\n",
      "    return list(cls(s))\n",
      "  File \"/home/wangshi/anaconda3/lib/python3.5/site-packages/dateutil/parser.py\", line 177, in __next__\n",
      "    token = self.get_token()\n",
      "  File \"/home/wangshi/anaconda3/lib/python3.5/site-packages/dateutil/parser.py\", line 93, in get_token\n",
      "    nextchar = self.instream.read(1)\n",
      "  File \"/home/wangshi/anaconda3/lib/python3.5/site-packages/pandas/core/generic.py\", line 2672, in __getattr__\n",
      "    return object.__getattribute__(self, name)\n",
      "AttributeError: 'Series' object has no attribute 'read'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-5-fbe3c663809a>\", line 62, in get_stock_report_list\n",
      "    get_eastmoney_stock_report(data_delta, DataFrame['文件夹'].loc[stock_code], DataFrame['证券代码'].loc[stock_code], DataFrame['证券名称'].loc[stock_code])\n",
      "  File \"<ipython-input-4-9480939d1263>\", line 33, in get_eastmoney_stock_report\n",
      "    print('Fail to get the file of ' + code + '_' + name + ',' + 'Please download the file manually !' + '\\n'  + temp_url_3)\n",
      "UnboundLocalError: local variable 'temp_url_3' referenced before assignment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to get reports of 600436_片仔癀\n",
      "\n",
      "Reports_Not_Exist:\n",
      "000004_国农科技\n",
      "000518_四环生物\n",
      "000590_启迪古汉\n",
      "000597_东北制药\n",
      "000766_通化金马\n",
      "000790_华神集团\n",
      "000989_九芝堂\n",
      "002166_莱茵生物\n",
      "002332_仙琚制药\n",
      "002680_长生生物\n",
      "600272_开开实业\n",
      "600380_健康元\n",
      "600671_天目药业\n",
      "600789_鲁抗医药\n",
      "600796_钱江生化\n",
      "600833_第一医药\n",
      "600851_海欣股份\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in industry_codes:\n",
    "    temp_list = get_sw_industry_list(i)\n",
    "    get_stock_report_list(temp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "USED 373 seconds!\n",
      "\n",
      "2016-09-02 01:01:20.805373\n"
     ]
    }
   ],
   "source": [
    "end_time = datetime.now()\n",
    "\n",
    "print('\\n' + 'USED ' + str((end_time - start_time).seconds) + ' seconds!')\n",
    "print('\\n' + str(datetime.now()))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
